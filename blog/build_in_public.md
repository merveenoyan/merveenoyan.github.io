# Build in Public

In [my previous post](../2025.md) I have mentioned about my lack of (feeling of) progress in French and how I burned out of it. Ever since that post I went to Alpes to celebrate xmas with my mother. I noticed during the entire trip I have spoken French to handle everything and it went smooth. I had an epiphany, I actually have come so far but I have absolutely zero feedback around it. I decided to take a step back on what I did in 2025 about my work and commit to new goals and build in public.

I committed this year to lower the barrier of entry for developers to build with open-source vision and multimodal models. These models are not complex as people think they are, but I noticed that people hesitate a little. To do this, I have built smol-vision and it was loved a ton by community! 

<img src="https://huggingface.co/datasets/merve/personal-website/resolve/main/smol-vision.png" alt="smol-vision" width="600">


In Hugging Face we have a culture of filling-in to where needed. At the beginning of the year there was a hype around using vision language models for agency. I have integrated vision capabilities to smolagents, we planned everything on how memory should work, how use cases like agentic browsers need to be written etc, it was super fun. Read more about it [here](https://huggingface.co/blog/smolagents-can-see). Similarly, I worked with TRL team to have more alignment methods for vision LMs ([read more](https://huggingface.co/blog/trl-vlm-alignment)).

We have written two major blogs around vision LMs this year, you might want to check them out. Last year we were trying to enable vision LM training in TRL, and to launch this, we have written a [blog](https://huggingface.co/blog/vlms) on vision LMs and how they work. This year we have written a follow up [blog](https://huggingface.co/blog/vlms-2025) on the paradigm changes (spoiler: it's a lot!). This year it was raining OCR models, and we were constantly receiving the question "which one is the best?" so we have written [a long post](https://huggingface.co/blog/ocr-open-models) on OCR landscape to help people get started easier.

I have written a book about vision language models with my friends in SmolVLM ðŸ“–

<div style="display: flex; gap: 10px; justify-content: space-between;">
  <img src="https://huggingface.co/datasets/merve/personal-website/resolve/main/book.png" alt="book" style="width: 30%;">
  <img src="https://huggingface.co/datasets/merve/personal-website/resolve/main/paper.png" alt="paper" style="width: 70%;">
</div>

This was a huge commitment. I got many questions from people whether it's worth writing a book or not. To be honest it takes almost all your weekends. This was something I wanted to do once in a lifetime. I learnt a lot from O'Reilly books when I was a junior data scientist, I even wrote [a blog](https://merveenoyan.medium.com/my-reviews-on-machine-learning-data-science-and-statistics-books-d1d70924b3d) on my honest take on those books. I wanted to give back in a way.

I have also worked on big model releases ([PaliGemma2](https://huggingface.co/blog/paligemma2mix), [Gemma-3/n](https://huggingface.co/merve/smol-vision/blob/main/Gemma3n_Fine_tuning_on_All_Modalities.ipynb), DINOv3 and more!), making fine-tuning scripts, helping out with transformers etc for cutting edge models. Getting my hands on shiny models is actually favorite part of my work, quite a gift.


I have done many other things on top of my responsibilities, but these ones were the largest changes I was proud of to lower barrier of entry to vision and multimodality.

This year **I commit to working on** distilling vision language models for vision tasks, more on-device/real-time vision and agentic multimodality, and continue making open vision more available to developers. I will make another blog next year to look back on this next year, I'm very excited. 

On top of this, I firmly believe in ever growing potential of Hugging Face Hub so I want to find ways to make Hub more adopted across modalities. Feel free to reach out to me at merve [at] huggingface [dot] co to give a feedback!